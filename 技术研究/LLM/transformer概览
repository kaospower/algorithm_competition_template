1.文本->token
2.embedding:token->向量的过程
3.模型为每个token加入位置信息:对于每个向量,它是什么,它在什么位置
4.QKV:将一句话抽象成词块数据库,数据库的查询键key就是词块本身,value就是词块本身的含义,Q,查询
5.注意力:Q.K点积计算相关程度,点积结果越大,相关性越高
6.如果向量维度较大,点积结果会整体偏大(假设服从正态分布,方差就是d),会导致某一个token的关注程度被放大的过于极端
在attention公式中,除以根号d进行注意力缩放scale,就可以防止这一点
7.结果用softmax进行作用,转化成权重,这些权重之和是1