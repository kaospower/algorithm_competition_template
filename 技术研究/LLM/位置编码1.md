1.位置编码(positional encoding)位于嵌入层之后,第一个编码器或是解码器层前  
2.什么是位置编码?  
首先通过tokenizer(Tokenizer分词)预处理输入词汇,将它分词成token序列,  
然后通过输入矩阵转换为词向量(词嵌入,word Embeddings),  
接着生成位置编码,把位置编码向量与词嵌入向量进行逐元素相加,  
最后把相加后的向量作为编码器和解码器层的输入   
3.为什么要在Transformer中引入它?  
注意力机制本身无法感知序列顺序  
Self-attention本质上是置换等变的(Permutation Equivariant):如果你打乱输入序列的顺序,输出序列会以完全相同的方式被  
一同打乱.  
$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$  
给AI模型一个关于顺序的"提示":即位置编码  
4.Transformer中的正余弦位置编码的数学公式?  
$PE_{(pos,2i)}=sin(\frac{pos}{10000^{2i/d_{model}}})$  
$PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{2i/d_{model}}})$  
pos是token在序列中的位置索引(从0开始)  
dmodel是模型的嵌入维度(如512)  
i是维度索引(取值范围:0<=i<$d_{model}$/2-1)  
$PE_{(pos,j)}$表示位置pos的编码向量中第j维的值  
位置编码:利用不同频率的正弦和余弦函数为每个位置生成一个独特的向量  
正弦和余弦函数的交替使用:  
对于每对相邻维度,偶数维度2i使用正弦函数,奇数维度2i+1使用余弦函数   
波长的递增序列:  
指数$10000^{2i/d_{model}}$为不同维度创建了不同的波长  
当i=0时:波长短,函数值变化快,有助于捕捉近距离关系,高敏感性   
当i=$d_{model}/2-1$时:波长长,在很长的序列范围内,高维度的编码值几乎恒定,有助于学习长距离依赖关系.   
稳定的,粗粒度的"区域"或"上下文"信号  
Why 10000?  
波长依据维度形成从$2\pi$到$10000\cdot 2\pi$的几何级数,保证位置编码的频率覆盖范围足够大,同时兼顾  
长序列与短序列   
5.这种编码方式有什么优势?  
唯一性与有界性:各位置的编码向量互不相同,所有值都在[-1,1]范围内,不需要学习,是确定性的.显示地引入了位置信息.  
无限外推能力:正余弦位置编码能够让模型外推到训练时未见过的序列长度.正余弦函数的连续性和无限定义域,函数sinx和cosx  
对于任何实数x都有定义,可以对任意位置索引生成编码,不受训练数据长度限制.编码模式沿着序列位置平滑变化,没有突变点.  
线性表示相对位置:任意固定位置偏移的相对位置关系可以被表达为线性变换.对于任意固定偏移k,$PE_{pos+k}$可以表示为$PE_{pos}$的线性函数.  
对于任意位置pos和偏移k,考虑特定维度i:令$\alpha=\frac{pos}{10000^{2i/d_{model}}}$和$\beta=\frac{k}{10000^{2i/d_{model}}}$,则  
$PE_{(pos,2i)}=sin(\frac{pos}{10000^{2i/d_{model}}})=sin\alpha$,$PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{2i/d_{model}}})=cos\alpha$  
利用三角函数两角和公式,得到偏移了k个位置之后的位置编码:  
偶数维度:$PE_{(pos+k,2i)}=sin(\alpha+\beta)=sin\alpha cos\beta+cos\alpha sin\beta$  
奇数维度:$PE_{(pos+k,2i+1)}=cos(\alpha+\beta)=cos\alpha cos\beta-sin\alpha sin\beta$  
偏移之后的位置编码可以通过原位置的位置编码的线性组合得到,线性系数仅仅和偏移量k有关,是独立于位置量的,  
高效推断相对关系,从绝对位置中去推断相对关系,但编码的仍然是每个token的绝对位置.  
6.旋转位置编码(RoPE)?  
在注意力机制计算中,对Query和Key向量做位置相关的二维旋转.  
把向量的维度两两配对,看作一系列二维平面.给对一个绝对位置p,对于第i对维度(2i,2i+1),定义一个旋转角:  
$\theta_{p,i}=p\cdot \omega_i$,其中$\omega_i=\frac{1}{base^{2i/d_{head}}}$,base是控制频率范围的超参数  
对于任意一个向量x(Q/K),其位于第i个二维平面上的分量$(x_{2i},x_{2i+1})$会被旋转$\theta_{p,i}$角度:  
$$
\begin{bmatrix}  
x_{2i}'\\
x_{2i+1}'\\
\end{bmatrix}=
\begin{bmatrix}
cos\theta_{p,i} & -sin\theta_{p,i}\\
sin\theta_{p,i} & cos\theta_{p,i}
\end{bmatrix}
\begin{bmatrix}
x_{2i}\\
x_{2i+1}
\end{bmatrix}
$$
等价于复数运算:将$x_{2i}+j\cdot x_{2i+1}$乘上$e^{j\theta_{p,i}}$  
当Query和Key都经过RoPE变换后,它们的注意力计算如下:  
$Attention(Q_m,K_n)=softmax(\frac{f(q,m)\cdot f(k,n)^T}{\sqrt d_k})$  
其中关键的内积计算变为:  
$f(q,m)\cdot f(k,n)=q\cdot k \cdot e^{i(m-n)\theta}$  
相对距离决定注意力:两个token之间的注意力分数只依赖于它们的相对位置差,而不是绝对位置   
平移不变性:序列整体平移不会改变内部的注意力模式  
保留长度外推能力:随着相对距离增加,注意力分数自然衰减  



