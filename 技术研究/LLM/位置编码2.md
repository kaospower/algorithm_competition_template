1.为什么要有位置编码?  
因为注意力模块本身不包含位置信息,位置编码在注意力计算过程中融入位置信息  
2.绝对位置编码?  
为每个位置分配一个单独的编码向量,然后加到原向量中  
可学习式(BERT,GPT等):为每个位置分配一个可以学习的向量,直接学习一个嵌入矩阵   
三角式(Transformer等):不同位置同一下标的元素近似呈周期分布,下标越大,周期越大,
2D旋转矩阵引入相对位置信息   
$PE_{(pos,2i)}=sin(\frac{pos}{10000^{2i/d_{model}}})$  
$PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{2i/d_{model}}})$  
$$
\mathop{\begin{bmatrix}  
PE_{(pos+\Delta,2i)}\\
PE_{(pos+\Delta,2i+1)}\\
\end{bmatrix}}\limits_{\vec{PE}_{pos+\Delta}}
=
\mathop{\begin{bmatrix}
cos(\Delta\theta_i) & sin(\Delta\theta_i)\\
-sin(\Delta\theta_i) & cos(\Delta\theta_i)
\end{bmatrix}}\limits_{旋转矩阵,顺时针旋转\Delta\theta_i(与具体的位置pos无关)}
\mathop{
\begin{bmatrix}
PE_{(pos,2i)}\\
PE_{(pos,2i+1)}
\end{bmatrix}}\limits_{\vec{PE}_{pos}}
$$  
旋转弧度正比于位置差$\Delta$,与具体的位置pos无关
