1.为什么要有位置编码?  
因为注意力模块本身不包含位置信息,位置编码在注意力计算过程中融入位置信息  
2.绝对位置编码?  
为每个位置分配一个单独的编码向量,然后加到原向量中  
可学习式(BERT,GPT等):为每个位置分配一个可以学习的向量,直接学习一个嵌入矩阵   
三角式(Transformer等):不同位置同一下标的元素近似呈周期分布,下标越大,周期越大,
2D旋转矩阵引入相对位置信息   
$PE_{(pos,2i)}=sin(\frac{pos}{10000^{2i/d_{model}}})$  
$PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{2i/d_{model}}})$  
$$
\mathop{\begin{bmatrix}  
PE_{(pos+\Delta,2i)}\\
PE_{(pos+\Delta,2i+1)}\\
\end{bmatrix}}\limits_{\vec{PE}_{pos+\Delta}}
=
\mathop{\begin{bmatrix}
cos(\Delta\theta_i) & sin(\Delta\theta_i)\\
-sin(\Delta\theta_i) & cos(\Delta\theta_i)
\end{bmatrix}}\limits_{旋转矩阵,顺时针旋转\Delta\theta_i(与具体的位置pos无关)}
\mathop{
\begin{bmatrix}
PE_{(pos,2i)}\\
PE_{(pos,2i+1)}
\end{bmatrix}}\limits_{\vec{PE}_{pos}}
$$  
旋转弧度正比于位置差$\Delta$,与具体的位置pos无关   

3.相对位置编码?   
相对位置编码不考虑绝对位置,在内积中融入相对位置信息.   
Attention  
$q_i=W_q(x_i+p_i)$  
$k_j=W_k(x_j+p_j)$  
$q_i^Tk_j=x_i^TW_q^TW_kx_j+\beta_{i-j}(相对位置项)$  
多头注意力:每个注意力头$head_h$分配一组偏置项$\beta_{i-j}^h$  

$T_5$:可学习偏置(分桶)  
最大位置差$(max_{dist}-1)$为15,总共分8个桶($num_{bucket}$)  
最后每个注意力头只需要学习一个8*1的向量:  
$$
head_h:PE_{8\times 1}^h=
\begin{bmatrix}
\beta_0^h\\
\beta_1^h\\
\vdots\\
\beta^h_7
\end{bmatrix}
$$
$$
Bucket_{id}(i-j)=\begin{cases}
i-j,& 0< i-j < \frac{num_{bucket}}{2}\\
(1+log(\frac{i-j}{\frac{num_{bucket}}{2}})/log(\frac{max_{dist}}{\frac{num_{bucket}}{2}}))\times \frac{num_{bucket}}{2},& i-j\ge\frac{num_{bucket}}{2}
\end{cases}
$$

ALiBi  
无需训练的线性偏置,在QK内积上直接加一个不用训练的偏置项:$softmax(q_iK^T+m\cdot[-(i-1),...,-2,-1,0])$   
偏置项大小正比于位置差,所以叫线性偏置  
m是每个注意力头的斜率:$m_h=2^{-\frac{8 \times h}{n_{head}}}$   

4.旋转位置编码?  
给每个位置分配一个绝对位置编码  
$q_m=f_q(x_{token},m)$  
$k_m=f_k(x_{token},m)$  
不同位置的qk内积又直接包含位置差的信息,内积直接包含相对位置(m-n)的信息   
$\langle f_q(x_m,m),f_k(x_n,n)\rangle=g(x_m,x_n,m-n)$  
$$
R_{m-n}=\begin{bmatrix}  
cos((m-n)\theta_i) & -sin((m-n)\theta_i)\\
sin((m-n)\theta_i) & cos((m-n)\theta_i)\\
\end{bmatrix}
$$
$R^T_nR_m=R_{m-n}$  
$(R_nq)^TR_mk=q^TR_{m-n}k$  
$\langle f_q(x_m,m),f_k(x_n,n)\rangle=g(x_m,x_n,m-n)$   

给词向量左乘一个位置m对应的旋转矩阵:$f_q(x_{token},m)=R_mW_qx_{token}$  
由于这里的$R_m$矩阵本身比较稀疏,直接用矩阵乘法实现效率较低,一种高效的等价实现方法:  
用矩阵元素相乘(Hadamard积)替代矩阵乘法,最后奇偶维度错位相加(同一组奇偶下标:同乘cos,同一组奇偶下标,符号相反乘sin)  





