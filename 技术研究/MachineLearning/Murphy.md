Machine Learning:A Probabilistic Perspective  
# prefix
The best way to make machines that can learn from data is to use the tools of probability theory.  
Bayesian approach  
maximum likelihood estimation  
# 1.Introduction
# 1.1 Machine learning:what and why?
long tail  
supervised learning:learn a mapping from inputs x to outputs y,given a labeled set of input-ouput pairs.  
classification:y is categorical  
regression:y is real-valued  
unsupervised learning:to find "interesting patterns" in the data(knowledge discovery)  
reinforcement learning:learning how to act or behave when given occasional reward or punishment signals  
# 1.2 Supervised learning
MAP(maximum a posteriori,最大后验) estimate  
CTR(click-through rate,点击率)  
bag of words  
exploratory data analysis  
object detection(CV计算机视觉领域,基于yolo框架,使用CNN技术)
Real-world applications:
1.Document classification and email spam filtering  
2.Classifying flowers  
3.Image classification and handwriting recognition  
4.Face detection and recognition  
# 1.3 Unsupervised learning

# 2.Probability
# 3.Generative models for discrete data
# 4.Gaussian models
# 5.Bayesian statistics
# 6.Frequentist statistics
# 7.Linear regression
# 8.Logistic regression
# 9.Generalized linear models and the exponential family
# 10.Directed graphical models(Bayes nets)
# 11.Mixture models and the EM algorithm
# 11.1 Latent variable models(潜变量模型)


# 12.Latent linear models
# 13.Sparse linear models
# 14.Kernels
# 15.Gaussian processes
# 16.Adaptive basis function models
# 17.Markov and hidden Markov models
# 18.State space models
# 19.Undirected graphical models(Markov random fields)
# 20.Exact inference for graphical models
# 21.Variational inference
# 22.More variational inference
# 23.Monte Carlo inference
# 24.Markov chain Monte Carlo(MCMC) inference
# 25.Clustering
# 26.Graphical model structure learning
# 27.Latent variable models for discrete data
# 28.Deep learning